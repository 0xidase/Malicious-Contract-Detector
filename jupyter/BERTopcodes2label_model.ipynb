{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venglov/.venv/main/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# from pyevmasm import disassemble_hex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# benign_df = pd.read_csv('../data/normal_df.csv').sample(frac=1).reset_index(drop=True)[:20000]\n",
    "# malicious_df = pd.read_csv('../data/malicious.csv')\n",
    "# sc_df = pd.concat([benign_df, malicious_df]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sc_df = pd.read_csv('/home/venglov/Documents/mcd_data/data_v2/opcodes_full_10k.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                            contract_address  \\\n0           0  0x0016394e785f667bb472d4148ab88a5c391accdf   \n1           1  0xbdc4dd4b2f62c446485a1d2e39569466a2fbd829   \n2           2  0x3e9ba4be6affa324f188b10962317239e32ce7ad   \n3           3  0xf5194c3325202f456c95c1cf0ca36f8475c1949f   \n4           4  0x5869d838be4f1efa10f2082efc957c79f7c3bf0b   \n\n                                             opcodes  malicious  \n0  PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x01 PUSH1 ...      False  \n1  PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x01 PUSH1 ...      False  \n2  PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x00 PUSH1 ...      False  \n3  PUSH1 0x60 PUSH2 0x375a PUSH2 0x0140 CODECOPY ...      False  \n4  PUSH1 0x80 PUSH1 0x40 MSTORE PUSH3 0x00001b PU...      False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>contract_address</th>\n      <th>opcodes</th>\n      <th>malicious</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0x0016394e785f667bb472d4148ab88a5c391accdf</td>\n      <td>PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x01 PUSH1 ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0xbdc4dd4b2f62c446485a1d2e39569466a2fbd829</td>\n      <td>PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x01 PUSH1 ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0x3e9ba4be6affa324f188b10962317239e32ce7ad</td>\n      <td>PUSH1 0x80 PUSH1 0x40 MSTORE PUSH1 0x00 PUSH1 ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0xf5194c3325202f456c95c1cf0ca36f8475c1949f</td>\n      <td>PUSH1 0x60 PUSH2 0x375a PUSH2 0x0140 CODECOPY ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0x5869d838be4f1efa10f2082efc957c79f7c3bf0b</td>\n      <td>PUSH1 0x80 PUSH1 0x40 MSTORE PUSH3 0x00001b PU...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# sc_df = sc_df[:10000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# sc_df = sc_df[['decompiled_opcodes', 'malicious']].sample(frac=1).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# sc_df = sc_df.drop(sc_df[sc_df.decompiled_code == \"error\"].index)\n",
    "# sc_df = sc_df.drop(sc_df[sc_df.decompiled_code == \"error ;\"].index)\n",
    "# sc_df = sc_df.reset_index()\n",
    "# sc_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# sc_df.opcodes.tolist()[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "inputs = tokenizer(sc_df.opcodes.tolist(), padding=\"max_length\", truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SmartContractsDataset(tc.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # self.sc_df = sc_df\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # code = self.sc_df.decompiled_code[idx]\n",
    "        # code = simple_web3_code_decompile(code)\n",
    "        # code = preprocess_code(code)\n",
    "        # code = self.get_contract_as_image(code).transpose(2, 0, 1)\n",
    "        # code = tc.tensor(code).type(tc.float32)\n",
    "        # code = tokenizer(code).items()\n",
    "\n",
    "        # mal = self.sc_df.malicious[idx]\n",
    "        # mal = tc.tensor(mal).type(tc.float32)\n",
    "        # return code, mal\n",
    "\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in code}\n",
    "        # item['label'] = mal\n",
    "        # return item\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [00:04<00:00, 54.1MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "device = \"cuda\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "scd = SmartContractsDataset(inputs, sc_df.malicious.tolist())\n",
    "train_size = int(0.8 * len(scd))\n",
    "val_size = int(0.1 * len(scd))\n",
    "test_size = len(scd) - train_size - val_size\n",
    "training_dataset, validation_dataset, testing_dataset = tc.utils.data.random_split(scd,\n",
    "                                                                                   [train_size, val_size, test_size])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    # use_mps_device=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venglov/.venv/main/lib64/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='5060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/5060 : < :, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=5060, training_loss=0.011367063457261323, metrics={'train_runtime': 891.3113, 'train_samples_per_second': 90.788, 'train_steps_per_second': 5.677, 'total_flos': 1.071926189924352e+16, 'train_loss': 0.011367063457261323, 'epoch': 10.0})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5330/1724829149.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(code[\"input_ids\"]).to(device)\n",
      "/tmp/ipykernel_5330/1724829149.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(code[\"attention_mask\"]).to(device)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(testing_dataset)):\n",
    "    code = testing_dataset[i]\n",
    "    label = code['label']\n",
    "    input_ids = torch.tensor(code[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(code[\"attention_mask\"]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    y = np.argmax(outputs[0].to('cpu').numpy())\n",
    "    results.append((label, y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 12\n",
      "True Negative: 1000\n",
      "False Positive: 0\n",
      "False Negative: 1\n",
      "Accuracy: 0.9990128331688055\n",
      "Precision: 1.0\n",
      "Recall: 0.9230769230769231\n",
      "F1-score 0.9600000000000001\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "# Generates count of inference quadrants\n",
    "for real, predicted in results:\n",
    "    if real == 1 and predicted == 1:\n",
    "        true_positive += 1\n",
    "    elif real == 0 and predicted == 0:\n",
    "        true_negative += 1\n",
    "    elif real == 1 and predicted == 0:\n",
    "        false_negative += 1\n",
    "    elif real == 0 and predicted == 1:\n",
    "        false_positive += 1\n",
    "\n",
    "# print(l2)\n",
    "print(\"True Positive:\", true_positive)\n",
    "print(\"True Negative:\", true_negative)\n",
    "print(\"False Positive:\", false_positive)\n",
    "print(\"False Negative:\", false_negative)\n",
    "\n",
    "# Machine Learning statistics and visuals. https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n",
    "print(\"Accuracy:\", (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative))\n",
    "precision = true_positive/(true_positive + false_positive)\n",
    "print(\"Precision:\", precision)\n",
    "recall = true_positive/(true_positive + false_negative)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score\", 2*(recall * precision)/(recall + precision))\n",
    "print(\"Specificity:\", true_negative/(true_negative + false_positive))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[(tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "real = []\n",
    "predicted = []\n",
    "for r, p in results:\n",
    "    real.append(r)\n",
    "    predicted.append(p)\n",
    "cf_matrix = confusion_matrix(real, predicted)\n",
    "df_cm = pd.DataFrame(cf_matrix, index=[0, 1],\n",
    "                     columns=[0, 1])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/venglov/Documents/mcd_data/data_v2/opcodes_model_weights_v2.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/venglov/Documents/mcd_data/data_v2/opcodes_model_weights_v2.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "address = \"0x792e8f3727cad6e00c58d478798f0907c4cec340\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "web3 = Web3(Web3.HTTPProvider(\"https://eth-mainnet.gateway.pokt.network/v1/lb/6266d6cdaa777e00391e0d29\"))\n",
    "code = web3.eth.getCode(Web3.toChecksumAddress(address))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "code = \"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\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "opcodes = disassemble_hex(code)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "opcodes = opcodes.replace('\\n', ' ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mopcodes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "inputs = tokenizer([opcodes], padding=\"max_length\", truncation=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mitems()\n",
      "File \u001B[0;32m~/miniconda3/envs/torch_transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:239\u001B[0m, in \u001B[0;36mBatchEncoding.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[item]\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encodings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndexing with integers (to access backend Encoding for a given batch index) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis not available when using Python based tokenizers\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    244\u001B[0m     )\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "inputs[2].items()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "item = {key: torch.tensor(val[0]) for key, val in inputs.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([  101,  5245,  2487,  1014,  2595, 17914,  5245,  2487,  1014,  2595,\n        12740,  5796, 19277,  5245,  2487,  1014,  2595,  2549,  2655,  2850,\n        10230,  4697,  8318,  5245,  2475,  1014,  2595, 27814,  5376,  2072,\n         5245,  2487,  1014,  2595,  2692,  2655,  2850,  9080, 10441,  2094,\n         5245,  2487,  1014,  2595,  2063,  2692, 14021,  2099,  4241,  2361,\n         2487,  5245,  2509,  1014,  2595,  2546,  2581, 16932,  3401,  1041,\n         4160,  5245,  2475,  1014,  2595,  2497,  2575,  5376,  2072,  4241,\n         2361,  2487,  5245,  2549,  1014,  2595,  2581,  2487,  2050, 19841,\n         2497,  2575,  2063,  1041,  4160,  5245,  2475,  1014,  2595, 20952,\n         5376,  2072,  4241,  2361,  2487,  5245,  2549,  1014,  2595,  2620,\n         2850,  2629, 27421,  2629,  2497,  1041,  4160,  5245,  2475,  1014,\n         2595, 14526,  2278,  5376,  2072,  4241,  2361,  2487,  5245,  2549,\n         1014,  2595,  2683,  2581,  7959,  2497,  2683, 23833,  1041,  4160,\n         5245,  2475,  1014,  2595, 16932,  2581,  5376,  2072,  4241,  2361,\n         2487,  5245,  2549,  1014,  2595,  2683, 18939,  2629, 18939,  2063,\n         2549,  1041,  4160,  5245,  2475,  1014,  2595, 16576,  2692,  5376,\n         2072,  4241,  2361,  2487,  5245,  2549,  1014,  2595,  2497,  2575,\n         2683, 12879,  2620,  2050,  2620,  1041,  4160,  5245,  2475,  1014,\n         2595, 16147,  2683,  5376,  2072,  5245,  2475,  1014,  2595,  2497,\n         2487,  5376,  5376,  6155,  2102,  2655,  2850, 10230,  4697,  5245,\n         2475,  1014,  2595,  2497,  2487,  5376,  2072,  2655, 10175,  5657,\n         5245,  2487,  1014,  2595,  2487,  5245,  2487,  1014,  2595,  2692,\n         4241,  2361,  2509,  4241,  2361,  2509, 22889, 10441,  2094,  5245,\n         2475,  1014,  2595,  2575,  2546, 19948,  2475, 19948,  2487,  5245,\n         2475,  1014,  2595, 28756,  2620,  5376,  5376,  6155,  2102, 19948,\n         2509,  3769,  3769,  4241,  2361,  2475, 19948,  2487,  7020, 19277,\n         3769,  5245, 16703,  1014,  2595,  2546,  2487,  2497,  2692,  2509,\n         2546, 19841,  2620,  2497,  2683,  2278, 23499,  2546, 19961,  2509,\n         7959,  2509,  2546,  2692,  3401,  2546,  2620, 23632, 21084,  2278,\n         2581,  2094,  2575,  2546,  2581, 20952,  2620, 21619, 20952,  2692,\n         2581,  2683,  2575,  2063,  2487,  2063,  2683,  2278,  2475,  9818,\n         2063,  2575,  4402, 23499,  2581,  2063, 20587,  2655, 10175,  5657,\n         5245,  2487,  1014,  2595, 12740, 19875, 10441,  2094,  5245,  2475,\n         1014, 18684,  2581, 19948,  2509, 19948,  2475, 19948,  2487,  5245,\n         2475,  1014,  2595,  2575,  8586,  5376,  5376,  6155,  2102,  5245,\n         2487,  1014,  2595, 12740, 19875, 10441,  2094,  4241,  2361,  2487,\n        19948,  2475,  4942, 19948,  2487,  8833,  2487,  2644,  5376,  6155,\n         2102,  5245,  2487,  1014,  2595,  2692,  4241,  2361,  2487,  7065,\n         8743,  5376,  6155,  2102,  2655, 10175,  5657,  4241,  2361,  2487,\n         2003,  6290,  2080,  5245,  2475,  1014,  2595,  2278,  2475,  5376,\n         2072,  5245,  2487,  1014,  2595,  2692,  4241,  2361,  2487,  7065,\n         8743,  5376,  6155,  2102,  3769,  5245,  2475,  1014,  2595, 14141,\n         5245,  2487,  1014,  2595,  2549,  4241,  2361,  2487,  2655,  2850,\n        10230,  4697,  4942,  4241,  2361,  2475,  5587, 19948,  2487,  5245,\n         2475,  1014,  2595,  2094,  2620, 19948,  2475, 19948,  2487,  5245,\n         2475,  1014,  2595,  2581,  2683,  2509,  5376,  5376,  6155,  2102,\n         5245,  2475,  1014,  2595,  2487,  2278,  2549,  5376,  5376,  6155,\n         2102,  2644,  5376,  6155,  2102,  2655, 10175,  5657,  4241,  2361,\n         2487,  2003,  6290,  2080,  5245,  2475,  1014,  2595, 15878,  5376,\n         2072,  5245,  2487,  1014,  2595,  2692,  4241,  2361,  2487,  7065,\n         8743,  5376,  6155,  2102,  3769,  5245,  2475,  1014,  2595, 10790,\n         2575,   102])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item['input_ids']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/97gp74_92k30dq5fwdw7j5bm0000gn/T/ipykernel_23062/1113231631.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item[\"input_ids\"]).to('cpu')\n",
      "/var/folders/tf/97gp74_92k30dq5fwdw7j5bm0000gn/T/ipykernel_23062/1113231631.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item[\"attention_mask\"]).to('cpu')\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(item[\"input_ids\"]).to('cpu')\n",
    "attention_mask = torch.tensor(item[\"attention_mask\"]).to('cpu')\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "y = np.argmax(outputs[0].to('cpu').numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-2.1050,  2.4778]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
