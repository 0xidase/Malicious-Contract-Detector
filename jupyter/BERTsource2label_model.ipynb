{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timeout_decorator'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmultiprocessing\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpanoramix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpanoramix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecompiler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decompile_bytecode\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DistilBertTokenizerFast\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "File \u001B[0;32m~/Main/Code/Malicious-Contract-Detector/jupyter/panoramix/panoramix/decompiler.py:9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m redirect_stdout\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtimeout_decorator\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpanoramix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfolder\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mfolder\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpanoramix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontract\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Contract\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'timeout_decorator'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch as tc\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re\n",
    "from panoramix.panoramix.decompiler import decompile_bytecode\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import web3\n",
    "from web3 import Web3\n",
    "\n",
    "\n",
    "def soft_decompile(byte_code):\n",
    "    try:\n",
    "        decompiled_code = decompile_bytecode(byte_code).text\n",
    "        assert len(decompiled_code) > 200\n",
    "        return decompiled_code\n",
    "    except:\n",
    "        try:\n",
    "            byte_code = '0x' + byte_code[66:]\n",
    "            decompiled_code = decompile_bytecode(byte_code).text\n",
    "            assert len(decompiled_code) > 200\n",
    "            return decompiled_code\n",
    "        except:\n",
    "            return \"error\"\n",
    "\n",
    "\n",
    "def force_web3_code_decompile(contract_address):\n",
    "    print(\"load from web3...\")\n",
    "    web3 = Web3(Web3.HTTPProvider(\"https://eth-mainnet.gateway.pokt.network/v1/lb/6266d6cdaa777e00391e0d29\"))\n",
    "    contract_bytecode = web3.eth.getCode(Web3.toChecksumAddress(contract_address))\n",
    "    contract_bytecode_string = contract_bytecode.hex()\n",
    "    decompiled_code = decompile_bytecode(contract_bytecode_string).text\n",
    "    return decompiled_code\n",
    "\n",
    "def simple_web3_code_decompile(byte_code):\n",
    "    try:\n",
    "        return decompile_bytecode(byte_code).text\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "def preprocess_code(code_str):\n",
    "    code_str = code_str.replace(\"\\n\\n\", \"\\n\")\n",
    "    lines = code_str.split('\\n')\n",
    "\n",
    "    # for line in lines:\n",
    "    #     print(line)\n",
    "\n",
    "    code_block = False\n",
    "    for i, line in enumerate(lines):\n",
    "\n",
    "        lines[i] += \" ;\"\n",
    "        line += \" ;\"\n",
    "        if line[:2] == \"  \" and not code_block:\n",
    "            code_block = True\n",
    "            lines[i] = line[2:]\n",
    "            lines[i - 1] = lines[i - 1].replace(':  ;', ' {')\n",
    "            lines[i - 1] = lines[i - 1].replace(': ;', ' {')\n",
    "        elif line[:2] == \"  \" and code_block:\n",
    "            lines[i] = line[2:]\n",
    "        elif line[:2] != \"  \" and code_block:\n",
    "            code_block = False\n",
    "            lines[i - 1] = lines[i - 1] + ' }'\n",
    "\n",
    "        lines[i] = lines[i].replace('(', ' ( ')\n",
    "        lines[i] = lines[i].replace(')', ' )')\n",
    "        lines[i] = lines[i].replace('[', ' [ ')\n",
    "        lines[i] = lines[i].replace(']', ' ]')\n",
    "        lines[i] = lines[i].replace('.', ' . ')\n",
    "        lines[i] = lines[i].replace('=', ' = ')\n",
    "\n",
    "    code_block = False\n",
    "\n",
    "    blocks_truncated = False\n",
    "    while not blocks_truncated:\n",
    "        blocks_truncated = True\n",
    "        for i, line in enumerate(lines):\n",
    "            if line[:2] == \"  \" and not code_block:\n",
    "                code_block = True\n",
    "                blocks_truncated = False\n",
    "                lines[i] = line[2:]\n",
    "                lines[i - 1] = lines[i - 1].replace(':  ;', ' {')\n",
    "                lines[i - 1] = lines[i - 1].replace(': ;', ' {')\n",
    "            elif line[:2] == \"  \" and code_block:\n",
    "                lines[i] = line[2:]\n",
    "            elif line[:2] != \"  \" and code_block:\n",
    "                code_block = False\n",
    "                lines[i - 1] = lines[i - 1] + ' }'\n",
    "\n",
    "    blocks_truncated = False\n",
    "    while not blocks_truncated:\n",
    "        blocks_truncated = True\n",
    "        for i, line in enumerate(lines):\n",
    "            if line[:2] == \" \" and not code_block:\n",
    "                code_block = True\n",
    "                blocks_truncated = False\n",
    "                lines[i] = line[2:]\n",
    "                lines[i - 1] = lines[i - 1].replace(':  ;', ' {')\n",
    "                lines[i - 1] = lines[i - 1].replace(': ;', ' {')\n",
    "            elif line[:2] == \" \" and code_block:\n",
    "                lines[i] = line[2:]\n",
    "            elif line[:2] != \" \" and code_block:\n",
    "                code_block = False\n",
    "                lines[i - 1] = lines[i - 1] + ' }'\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        lines[i] = line.replace(\"  \", \" \")\n",
    "    # for line in lines:\n",
    "    #     print(line)\n",
    "\n",
    "    joined =  \" \".join(lines)\n",
    "    pattern_code = r\"0x[A-Fa-f0-9]{41,}\\s\"\n",
    "    pattern_address = r\"0x[A-Fa-f0-9]{40}\\s\"\n",
    "    result = re.sub(pattern_code, \"CODE\", joined)\n",
    "    result = re.sub(pattern_address, \"ADDRESS\", result)\n",
    "\n",
    "    return result\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Timed out!\")\n",
    "\n",
    "def decompile_and_preprocess_row_target(bytecode_from_web3, creation_bytecode, return_dict):\n",
    "    try:\n",
    "\n",
    "        # if count % 10 == 0:\n",
    "        #     print(count)\n",
    "\n",
    "        if bytecode_from_web3 == '0x':\n",
    "            return_dict['result'] = preprocess_code(soft_decompile(creation_bytecode))\n",
    "        else:\n",
    "            return_dict['result'] = preprocess_code(simple_web3_code_decompile(bytecode_from_web3))\n",
    "    except:\n",
    "        return_dict['result'] = \"error\"\n",
    "\n",
    "\n",
    "def decompile_and_preprocess_row(bytecode_from_web3, creation_bytecode, malicious):\n",
    "    if malicious == 0:\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "\n",
    "        p = multiprocessing.Process(target=decompile_and_preprocess_row_target, args=(bytecode_from_web3, creation_bytecode, return_dict))\n",
    "        p.start()\n",
    "        p.join(60)\n",
    "        if p.is_alive():\n",
    "            print(\"running... let's kill it...\")\n",
    "            p.terminate()\n",
    "            p.join()\n",
    "            return \"error\"\n",
    "        else:\n",
    "            return return_dict['result']\n",
    "    elif malicious == 1:\n",
    "        return_dict = {}\n",
    "        decompile_and_preprocess_row_target(bytecode_from_web3, creation_bytecode, return_dict)\n",
    "        return return_dict['result']\n",
    "    else:\n",
    "        return \"error\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "normal_with_code_df = pd.read_csv('/home/venglov/Documents/normal_with_code.csv')\n",
    "malicious_with_code_df = pd.read_csv('/home/venglov/Documents/malicious_with_code.csv')\n",
    "sc_df = pd.concat([normal_with_code_df, malicious_with_code_df]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n0                0         16804         16804       16804   \n1                1         15728         15728       15728   \n2                2          6017          6017        6017   \n3                3         27433         27433       27433   \n4                4         28213         28213       28213   \n...            ...           ...           ...         ...   \n3136           136           136           136         136   \n3137           137           137           137         137   \n3138           138           138           138         138   \n3139           139           139           139         139   \n3140           140           140           140         140   \n\n                                contract_address       contract_name  \\\n0     0xb9feb10ec48f1377e3232504c8d55d8b829e7925                 ASC   \n1     0xc9830b80b9a8d88a30919cda303b2cef9b7e7ab3  HumanStandardToken   \n2     0x2f1817f25d95f03355fdc5f55d91aec7bc94db43           Forwarder   \n3     0x767820dd0636ea9fa6b26a5be71b93b9f5ed79eb               BPool   \n4     0x1e8bbde11dfef31532c78260502ae0a629f58e39     CEtherDelegator   \n...                                          ...                 ...   \n3136  0x10c509aa9ab291c76c45414e7cdbd375e1d5ace8                 NaN   \n3137  0xdba351f902c8cd525329e2d38b0e1e8343a0d7b5                 NaN   \n3138  0x54b5ae5ebe86d2d86134f3bb7e36e7c83295cbcb                 NaN   \n3139  0xf52045f26afece501048e132e5c9544c0b6ece94                 NaN   \n3140  0x7b9175d5c08642c6c2abd418a279135961be52b7                 NaN   \n\n     contract_etherscan_label          contract_tag  \\\n0                         NaN                   NaN   \n1                         NaN                   NaN   \n2                         NaN                   NaN   \n3                         NaN                   NaN   \n4                         NaN                   NaN   \n...                       ...                   ...   \n3136                      NaN                   NaN   \n3137                      NaN                   NaN   \n3138                      NaN  BT.Finance Exploiter   \n3139                      NaN                   NaN   \n3140                      NaN                   NaN   \n\n                                contract_creator  \\\n0     0x6ace97b861e902ce9076d6de027c372862f9d122   \n1     0x8063b2b7374dd4a24d6324fbabebfdfc5d7868bc   \n2     0x00bdb5699745f5b860228c8f939abf1b9ae374ed   \n3     0xd3ec472688fe68c3398fa4ead35b4d97e63031bc   \n4     0x6ce798bc8c8c93f3c312644dcbdd2ad6698622c5   \n...                                          ...   \n3136  0x8efab89b497b887cdaa2fb08ff71e4b3827774b2   \n3137  0x941a9e3b91e1cc015702b897c512d265fae88a9c   \n3138  0x358abccb4f5bb715482271890929fdabb3015878   \n3139  0x8b4c1083cd6aef062298e1fa900df9832c8351b3   \n3140  0xba5ed1488be60ba2facc6b66c6d6f0befba22ebe   \n\n                                   contract_creation_tx  ... malicious  \\\n0     0x4246c9aef0d1a0a13bd870292940eb06737529383cf5...  ...         0   \n1     0x113b447dfe61c19fe0c8eafd86806aa683c4c25f9a2d...  ...         0   \n2     0x3520c3e2252bf6c2c4e421e9a00ec43b17dd4a1944ee...  ...         0   \n3     0x3fc842f9c19e8bfde3c25b938372be52372a67f1cfde...  ...         0   \n4     0x23e434e7fe104c070faf16e10ad8de42a44afe739709...  ...         0   \n...                                                 ...  ...       ...   \n3136  0xbe65cb0dd9f4619939cfeb56b3ef3a996e2b028b93fd...  ...         1   \n3137  0x7f777a94444ff4c85b00698e918e5b340afdd6d6ae95...  ...         1   \n3138  0xf792f288e0e23e0d65136de3f309572e29890735586c...  ...         1   \n3139  0x39982143e4c82fc1686948b4ccc50135256329b105f9...  ...         1   \n3140  0xf57a9218cf3ce054c1770db0206594860663d33d8dce...  ...         1   \n\n             contract_creator_tag                       source notes  \\\n0                             NaN                          NaN   NaN   \n1                             NaN                          NaN   NaN   \n2                             NaN                          NaN   NaN   \n3                             NaN                          NaN   NaN   \n4                             NaN                          NaN   NaN   \n...                           ...                          ...   ...   \n3136      Visor Finance Exploiter  Luabase ethereum.tags table   NaN   \n3137          ChainSwap Exploiter  Luabase ethereum.tags table   NaN   \n3138                          NaN  Luabase ethereum.tags table   NaN   \n3139  Inverse Finance Exploiter 2  Luabase ethereum.tags table   NaN   \n3140    Indexed Finance Exploiter  Luabase ethereum.tags table   NaN   \n\n     contract_creator_etherscan_label __index_level_0__       _lua_timestamp  \\\n0                                 NaN             56743  2022-12-05T20:18:10   \n1                                 NaN            119161  2022-12-05T20:18:10   \n2                                 NaN            105643  2022-12-05T20:18:10   \n3                                 NaN             99616  2022-12-05T20:18:10   \n4                                 NaN             87448  2022-12-05T20:18:10   \n...                               ...               ...                  ...   \n3136                            heist                74  2022-12-05T20:18:10   \n3137                          exploit                79  2022-12-05T20:18:10   \n3138                              NaN                19  2022-12-05T20:18:10   \n3139                          exploit                66  2022-12-05T20:18:10   \n3140                            heist               105  2022-12-05T20:18:10   \n\n                                 _lua_uuid  \\\n0     bdbf408d-2097-47bd-846e-5af7a8db454c   \n1     df4700a2-befe-4921-8409-bc28bb58d148   \n2     db408986-a069-4e14-8318-07a783d0bfb7   \n3     3ecf7999-027c-4649-8e3a-133028e71f97   \n4     4feeea4d-3c3a-42e4-8fb8-4c3f3586df73   \n...                                    ...   \n3136  cd25dc25-4a41-44cc-b936-9c88805d1e25   \n3137  b0ce3a52-7134-452a-bc61-43d92c599994   \n3138  45427982-d406-44ee-ba8d-c3dbdf3418d9   \n3139  700d78a5-c3a8-49c7-bcae-8ee968bc27a9   \n3140  600047d3-e14f-4fc3-b9c2-5cb1d0c8ce72   \n\n                                     bytecode_from_web3  \\\n0     0x608060405260043610610099576000357c0100000000...   \n1     0x6060604052361561008d5760e060020a600035046306...   \n2     0x6060604052361561003a5763ffffffff60e060020a60...   \n3     0x363d3d373d3d3d363d73ee8ac082aeb41b0f5a41aae8...   \n4     0x6080604052600436106100555760003560e01c806309...   \n...                                                 ...   \n3136  0x608060405234801561001057600080fd5b5060043610...   \n3137  0x608060405234801561001057600080fd5b5060043610...   \n3138                                                 0x   \n3139  0x60806040526004361061007f5760003560e01c806349...   \n3140  0x60806040526004361061005e5760003560e01c806315...   \n\n                                        decompiled_code  \n0     def storage { balanceOf is mapping of uint256 ...  \n1     def storage { balanceOf is mapping of uint256 ...  \n2     def storage { parentAddress is addr at storage...  \n3     def _fallback ( ) payable { delegate ADDRESSwi...  \n4     def storage { implementationAddress is addr at...  \n...                                                 ...  \n3136  def storage { owner is addr at storage 0 ; poo...  \n3137  def storage { owner is addr at storage 0 ; } d...  \n3138  def storage { stor0 is addr at storage 0 ; sto...  \n3139  def storage { stor0 is uint256 at storage 0 ; ...  \n3140  def storage { stor3 is addr at storage 3 ; sto...  \n\n[3141 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0.3</th>\n      <th>Unnamed: 0.2</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0</th>\n      <th>contract_address</th>\n      <th>contract_name</th>\n      <th>contract_etherscan_label</th>\n      <th>contract_tag</th>\n      <th>contract_creator</th>\n      <th>contract_creation_tx</th>\n      <th>...</th>\n      <th>malicious</th>\n      <th>contract_creator_tag</th>\n      <th>source</th>\n      <th>notes</th>\n      <th>contract_creator_etherscan_label</th>\n      <th>__index_level_0__</th>\n      <th>_lua_timestamp</th>\n      <th>_lua_uuid</th>\n      <th>bytecode_from_web3</th>\n      <th>decompiled_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>16804</td>\n      <td>16804</td>\n      <td>16804</td>\n      <td>0xb9feb10ec48f1377e3232504c8d55d8b829e7925</td>\n      <td>ASC</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x6ace97b861e902ce9076d6de027c372862f9d122</td>\n      <td>0x4246c9aef0d1a0a13bd870292940eb06737529383cf5...</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>56743</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>bdbf408d-2097-47bd-846e-5af7a8db454c</td>\n      <td>0x608060405260043610610099576000357c0100000000...</td>\n      <td>def storage { balanceOf is mapping of uint256 ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>15728</td>\n      <td>15728</td>\n      <td>15728</td>\n      <td>0xc9830b80b9a8d88a30919cda303b2cef9b7e7ab3</td>\n      <td>HumanStandardToken</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x8063b2b7374dd4a24d6324fbabebfdfc5d7868bc</td>\n      <td>0x113b447dfe61c19fe0c8eafd86806aa683c4c25f9a2d...</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>119161</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>df4700a2-befe-4921-8409-bc28bb58d148</td>\n      <td>0x6060604052361561008d5760e060020a600035046306...</td>\n      <td>def storage { balanceOf is mapping of uint256 ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>6017</td>\n      <td>6017</td>\n      <td>6017</td>\n      <td>0x2f1817f25d95f03355fdc5f55d91aec7bc94db43</td>\n      <td>Forwarder</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x00bdb5699745f5b860228c8f939abf1b9ae374ed</td>\n      <td>0x3520c3e2252bf6c2c4e421e9a00ec43b17dd4a1944ee...</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>105643</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>db408986-a069-4e14-8318-07a783d0bfb7</td>\n      <td>0x6060604052361561003a5763ffffffff60e060020a60...</td>\n      <td>def storage { parentAddress is addr at storage...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>27433</td>\n      <td>27433</td>\n      <td>27433</td>\n      <td>0x767820dd0636ea9fa6b26a5be71b93b9f5ed79eb</td>\n      <td>BPool</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0xd3ec472688fe68c3398fa4ead35b4d97e63031bc</td>\n      <td>0x3fc842f9c19e8bfde3c25b938372be52372a67f1cfde...</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>99616</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>3ecf7999-027c-4649-8e3a-133028e71f97</td>\n      <td>0x363d3d373d3d3d363d73ee8ac082aeb41b0f5a41aae8...</td>\n      <td>def _fallback ( ) payable { delegate ADDRESSwi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>28213</td>\n      <td>28213</td>\n      <td>28213</td>\n      <td>0x1e8bbde11dfef31532c78260502ae0a629f58e39</td>\n      <td>CEtherDelegator</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x6ce798bc8c8c93f3c312644dcbdd2ad6698622c5</td>\n      <td>0x23e434e7fe104c070faf16e10ad8de42a44afe739709...</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>87448</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>4feeea4d-3c3a-42e4-8fb8-4c3f3586df73</td>\n      <td>0x6080604052600436106100555760003560e01c806309...</td>\n      <td>def storage { implementationAddress is addr at...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3136</th>\n      <td>136</td>\n      <td>136</td>\n      <td>136</td>\n      <td>136</td>\n      <td>0x10c509aa9ab291c76c45414e7cdbd375e1d5ace8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x8efab89b497b887cdaa2fb08ff71e4b3827774b2</td>\n      <td>0xbe65cb0dd9f4619939cfeb56b3ef3a996e2b028b93fd...</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Visor Finance Exploiter</td>\n      <td>Luabase ethereum.tags table</td>\n      <td>NaN</td>\n      <td>heist</td>\n      <td>74</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>cd25dc25-4a41-44cc-b936-9c88805d1e25</td>\n      <td>0x608060405234801561001057600080fd5b5060043610...</td>\n      <td>def storage { owner is addr at storage 0 ; poo...</td>\n    </tr>\n    <tr>\n      <th>3137</th>\n      <td>137</td>\n      <td>137</td>\n      <td>137</td>\n      <td>137</td>\n      <td>0xdba351f902c8cd525329e2d38b0e1e8343a0d7b5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x941a9e3b91e1cc015702b897c512d265fae88a9c</td>\n      <td>0x7f777a94444ff4c85b00698e918e5b340afdd6d6ae95...</td>\n      <td>...</td>\n      <td>1</td>\n      <td>ChainSwap Exploiter</td>\n      <td>Luabase ethereum.tags table</td>\n      <td>NaN</td>\n      <td>exploit</td>\n      <td>79</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>b0ce3a52-7134-452a-bc61-43d92c599994</td>\n      <td>0x608060405234801561001057600080fd5b5060043610...</td>\n      <td>def storage { owner is addr at storage 0 ; } d...</td>\n    </tr>\n    <tr>\n      <th>3138</th>\n      <td>138</td>\n      <td>138</td>\n      <td>138</td>\n      <td>138</td>\n      <td>0x54b5ae5ebe86d2d86134f3bb7e36e7c83295cbcb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>BT.Finance Exploiter</td>\n      <td>0x358abccb4f5bb715482271890929fdabb3015878</td>\n      <td>0xf792f288e0e23e0d65136de3f309572e29890735586c...</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Luabase ethereum.tags table</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>19</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>45427982-d406-44ee-ba8d-c3dbdf3418d9</td>\n      <td>0x</td>\n      <td>def storage { stor0 is addr at storage 0 ; sto...</td>\n    </tr>\n    <tr>\n      <th>3139</th>\n      <td>139</td>\n      <td>139</td>\n      <td>139</td>\n      <td>139</td>\n      <td>0xf52045f26afece501048e132e5c9544c0b6ece94</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0x8b4c1083cd6aef062298e1fa900df9832c8351b3</td>\n      <td>0x39982143e4c82fc1686948b4ccc50135256329b105f9...</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Inverse Finance Exploiter 2</td>\n      <td>Luabase ethereum.tags table</td>\n      <td>NaN</td>\n      <td>exploit</td>\n      <td>66</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>700d78a5-c3a8-49c7-bcae-8ee968bc27a9</td>\n      <td>0x60806040526004361061007f5760003560e01c806349...</td>\n      <td>def storage { stor0 is uint256 at storage 0 ; ...</td>\n    </tr>\n    <tr>\n      <th>3140</th>\n      <td>140</td>\n      <td>140</td>\n      <td>140</td>\n      <td>140</td>\n      <td>0x7b9175d5c08642c6c2abd418a279135961be52b7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0xba5ed1488be60ba2facc6b66c6d6f0befba22ebe</td>\n      <td>0xf57a9218cf3ce054c1770db0206594860663d33d8dce...</td>\n      <td>...</td>\n      <td>1</td>\n      <td>Indexed Finance Exploiter</td>\n      <td>Luabase ethereum.tags table</td>\n      <td>NaN</td>\n      <td>heist</td>\n      <td>105</td>\n      <td>2022-12-05T20:18:10</td>\n      <td>600047d3-e14f-4fc3-b9c2-5cb1d0c8ce72</td>\n      <td>0x60806040526004361061005e5760003560e01c806315...</td>\n      <td>def storage { stor3 is addr at storage 3 ; sto...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3141 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_df = sc_df[:3141]\n",
    "sc_df = sc_df.drop(sc_df[sc_df.decompiled_code == \"error\"].index)\n",
    "sc_df = sc_df.drop(sc_df[sc_df.decompiled_code == \"error ;\"].index)\n",
    "sc_df = sc_df.reset_index()\n",
    "sc_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "inputs = tokenizer(sc_df.decompiled_code.tolist(), padding=\"max_length\", truncation=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class SmartContractsDataset(tc.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # self.sc_df = sc_df\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # code = self.sc_df.decompiled_code[idx]\n",
    "        # code = simple_web3_code_decompile(code)\n",
    "        # code = preprocess_code(code)\n",
    "        # code = self.get_contract_as_image(code).transpose(2, 0, 1)\n",
    "        # code = tc.tensor(code).type(tc.float32)\n",
    "        # code = tokenizer(code).items()\n",
    "\n",
    "        # mal = self.sc_df.malicious[idx]\n",
    "        # mal = tc.tensor(mal).type(tc.float32)\n",
    "        # return code, mal\n",
    "\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in code}\n",
    "        # item['label'] = mal\n",
    "        # return item\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx])\n",
    "        return item\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "scd = SmartContractsDataset(inputs, sc_df.malicious.tolist())\n",
    "train_size = int(0.8 * len(scd))\n",
    "val_size = int(0.1 * len(scd))\n",
    "test_size = len(scd) - train_size - val_size\n",
    "training_dataset, validation_dataset, testing_dataset = tc.utils.data.random_split(scd,\n",
    "                                                                                   [train_size, val_size, test_size])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venglov/.venv/torch_gpu/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2070\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2600\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2600 : < :, Epoch 0.01/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2600, training_loss=0.03126139126640807, metrics={'train_runtime': 463.7623, 'train_samples_per_second': 89.27, 'train_steps_per_second': 5.606, 'total_flos': 5484150304358400.0, 'train_loss': 0.03126139126640807, 'epoch': 20.0})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1000348/1724829149.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(code[\"input_ids\"]).to(device)\n",
      "/tmp/ipykernel_1000348/1724829149.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(code[\"attention_mask\"]).to(device)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(testing_dataset)):\n",
    "    code = testing_dataset[i]\n",
    "    label = code['label']\n",
    "    input_ids = torch.tensor(code[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(code[\"attention_mask\"]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    y = np.argmax(outputs[0].to('cpu').numpy())\n",
    "    results.append((label, y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 14\n",
      "True Negative: 244\n",
      "False Positive: 0\n",
      "False Negative: 2\n",
      "Accuracy: 0.9923076923076923\n",
      "Precision: 1.0\n",
      "Recall: 0.875\n",
      "F1-score 0.9333333333333333\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "# Generates count of inference quadrants\n",
    "for real, predicted in results:\n",
    "    if real == 1 and predicted == 1:\n",
    "        true_positive += 1\n",
    "    elif real == 0 and predicted == 0:\n",
    "        true_negative += 1\n",
    "    elif real == 1 and predicted == 0:\n",
    "        false_negative += 1\n",
    "    elif real == 0 and predicted == 1:\n",
    "        false_positive += 1\n",
    "\n",
    "# print(l2)\n",
    "print(\"True Positive:\", true_positive)\n",
    "print(\"True Negative:\", true_negative)\n",
    "print(\"False Positive:\", false_positive)\n",
    "print(\"False Negative:\", false_negative)\n",
    "\n",
    "# Machine Learning statistics and visuals. https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n",
    "print(\"Accuracy:\", (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative))\n",
    "precision = true_positive/(true_positive + false_positive)\n",
    "print(\"Precision:\", precision)\n",
    "recall = true_positive/(true_positive + false_negative)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score\", 2*(recall * precision)/(recall + precision))\n",
    "print(\"Specificity:\", true_negative/(true_negative + false_positive))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "[(tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(0), 0),\n (tensor(1), 1),\n (tensor(0), 0),\n (tensor(0), 0)]"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot: >"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x700 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "real = []\n",
    "predicted = []\n",
    "for r, p in results:\n",
    "    real.append(r)\n",
    "    predicted.append(p)\n",
    "cf_matrix = confusion_matrix(real, predicted)\n",
    "df_cm = pd.DataFrame(cf_matrix, index=[0, 1],\n",
    "                     columns=[0, 1])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/venglov/Documents/model_weights.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/venglov/Documents/model_weights.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "address = \"0x792e8f3727cad6e00c58d478798f0907c4cec340\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/venglov/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/venglov/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/venglov/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/venglov/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "web3 = Web3(Web3.HTTPProvider(\"https://eth-mainnet.gateway.pokt.network/v1/lb/6266d6cdaa777e00391e0d29\"))\n",
    "code = web3.eth.getCode(Web3.toChecksumAddress(address))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "code = \"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\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'disassemble_hex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m opcodes \u001B[38;5;241m=\u001B[39m \u001B[43mdisassemble_hex\u001B[49m(code)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'disassemble_hex' is not defined"
     ]
    }
   ],
   "source": [
    "opcodes = disassemble_hex(code)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\"def _fallback ( ) payable {  . . .  # Decompilation aborted, sorry: ( 'jump to a parameter computed at runtime', 0 ) ; }  ;\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "opcodes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "inputs = tokenizer([decompiled], padding=\"max_length\", truncation=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "item = {key: torch.tensor(val[0]) for key, val in inputs.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1000348/3075049496.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item[\"input_ids\"]).to(device)\n",
      "/tmp/ipykernel_1000348/3075049496.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item[\"attention_mask\"]).to(device)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(item[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(item[\"attention_mask\"]).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "y = np.argmax(outputs[0].to('cpu').numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 4.8980, -4.9567]], device='cuda:0')"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
